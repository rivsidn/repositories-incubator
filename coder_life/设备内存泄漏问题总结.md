## 现象描述

* 设备持续性内存增加，大约每天2%
* 通过查看`/proc/slabinfo` 没有发现有明显增加
* 查看用户态进程没有发现有明显增加
* 后台有输出`1519` `1524` 等...



## 分析定位

### STEP 1

鉴于以上分析，怀疑排除`slab` 内存泄漏、用户态进程内存泄漏可能性，分析可能是底层直接通过`alloc_pages()`申请的内存导致内存泄漏。

* 内核开启`PAGE_OWNER` 选项查看页面申请增加最多的部分为某国产网卡，但是除了该网卡之外，其他网卡也存在增加的情况

基于该现象，怀疑是软件导致的内存泄漏，可能原因是增加了`sk_buff` 引用计数没有释放导致的`skb` `skb->data`没有释放，因为所有网卡`skb`都出现了增加。



### STEP 2

现场环境将某国产网卡替换为`intel` 网卡之后，持续观察一段时间，内存泄漏问题不存在；换到热备机上之后，又再次出现`1524` 等输出，内存持续增减。

此时 **国产网卡驱动** 为重点怀疑对象，怀疑`1524`输出为报文长度，参照这个思路找驱动代码，确实在收大于`1518`的包时候会有类似输出。

* 联系网卡厂商排查，却分当代码走到这个分支时，是否会出现内存泄漏
* 修改`MTU`，发送大包，查看是否会有输出，内存是否会增加

经过上边确认之后，确实是网卡驱动存在内存泄漏。



## 总结

回顾分析过程中出现的几个错误：

1. 没有列一个表格，将所有线索整理到一起，整理思路按照可能性高低排序，**将大脑中的第一印象作为既定结论继续往下走，最后才发现走歪了**。

   类似的是，认为`1519`、`1524` 是调试过程中输出的行号，忽视了两个点：

   * 这里不像行号，行号输出不会这么近
   * 只有异常情况下出现了这种现象，正常情况下没出现类似现象

   这里这个现象是需要密切关注的，但是由于第一印象是行号，所以就忽略掉了。

2. 思路不够`open`，不认为硬件、驱动会有问题，优先考虑软件问题。

   这种思路，大多数时候不能说是坏事，问题分析过程中优先排除自己这边，再继续向下分析。

   但是注意到，这时候解决问题时，顺序是线性的，先软件、再硬件、类推，这种方式分析问题速度不快，有时候步子大一点问题解决的会更快。

3. 基础知识并不充分，这里体现在，`PAGE_OWNER` 查看时候，不仅国产网卡申请的内存会增加，其他网卡同样会增加，不明白这里是怎么回事，导致思路怀疑到软件上去。

   问题分析过程中，总会有不是很明晰的点，这里要么去确认，要不可以采取某种分析方法，让自己找到正确的问题所在。



## TODO

* `sk_buff` 在内核中的流程，那些占用的`sk_buff` 是做什么用的
* 为什么`page owner`发现其他网卡申请的内存也会增加
* 调用栈的输出，为什么会出现交叉的情况
